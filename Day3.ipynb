{
 "cells": [
  {
   "attachments": {
    "09eb7b40-03ff-431c-85cc-b07ca6018efd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "82beddbb-1616-413c-9c08-78aea57f2ea5",
   "metadata": {},
   "source": [
    "![ecole.png](attachment:09eb7b40-03ff-431c-85cc-b07ca6018efd.png)\n",
    "### Higher School of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83bc2a-5f24-470b-b783-08ff1c65229b",
   "metadata": {},
   "source": [
    "# ğŸ“š Machine Learning on Financial Data\n",
    "**Prerequisites**\n",
    "\n",
    "[Data Types in Data Science for Financial Analysis](Day2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357a447-3778-4c9b-9e04-9ac057e0430e",
   "metadata": {},
   "source": [
    "Machine learning plays a transformative role in financial data analysis by enabling automated decision-making and predictive modeling; classification algorithms like Decision Trees and SVMs are widely used to identify patterns and categorize financial recordsâ€”such as credit approval, fraud detection, or risk profilingâ€”based on historical features, while regression models utilizing the same techniques help forecast continuous variables like stock prices, interest rates, or portfolio returns, allowing institutions to anticipate market trends and optimize strategies with improved accuracy and efficiency.\n",
    "\n",
    "This notebook introduces the application of machine learning algorithms to the two types of financial data :\n",
    "- Classification on tabular data,\n",
    "- Regresion on time series data.\n",
    "\n",
    "To perform both classification and regression tasks. Specifically, it demonstrates how models like Decision Trees and Support Vector Machines (SVMs) can be applied to structured tabular data for predicting categorical outcomes such as loan approval status; how those same models, can be leveraged for time series regression to forecast numerical financial values such as stock prices â€” highlighting the versatility of machine learning across diverse financial data types and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e823cb-e849-4fe0-b8bd-ede6b8b17820",
   "metadata": {},
   "source": [
    "# ğŸ“Š Tabular Financial Data for Classification\n",
    "\n",
    "In the context of the financial dataset loan_approval_dataset.csv, classification refers to the task of predicting whether a loan application will be approved or rejected, based on various applicant features provided in the dataset.\n",
    "\n",
    "The goal is to build a model that learns patterns from historical data to accurately assign each new application to one of two classes:\n",
    "- Approved (1 or \"Y\")\n",
    "- Rejected (0 or \"N\")\n",
    "\n",
    "The model is a decision making system used to automate the loan approval process and identify high-risk applications early.\n",
    "\n",
    "ğŸ§  Used Models\n",
    "\n",
    "- Decision Tree: Easy to interpret rule-based model that splits data into subgroups,\n",
    "- SVM (Support Vector Machine): Powerful model that finds an optimal boundary between approval and rejection.\n",
    "\n",
    "ğŸ” Objective: Train and evaluate these models on historical loan data to predict future approvals and improve loan portfolio quality.\n",
    "\n",
    "## ğŸ§  Objective:\n",
    "- Step 1 â€“ Load and Preprocess the Tabular Financial Dataset,\n",
    "- Step 2 â€“ Decision tree data classification,\n",
    "- Step 3 â€“ Support Vector Machine (SVM) data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968661fa-e5ea-469a-9e3d-7fc55574199a",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 1 â€“ Load and Preprocess the Tabular Financial Dataset\n",
    "### ğŸ¦ About Dataset\n",
    "The loan_approval_dataset.csv dataset comprises historical records of loan applications collected from a financial institution. It contains demographic, financial, and asset-related information of applicants, used to take the final loan approval decision.\n",
    "\n",
    "The dataset includes a mix of numerical and categorical variables used to make the final decision located at the last column :\n",
    "\n",
    "- ApplicantIncome â€“ Monthly income of the applicant,\n",
    "- CoapplicantIncome â€“ Income of a co-applicant, if any,\n",
    "- LoanAmount â€“ Amount of the requested loan,\n",
    "- Credit_History â€“ Binary variable indicating past creditworthiness,\n",
    "- Gender, Married, Dependents, Education, Self_Employed â€“ Demographic and employment-related features,\n",
    "- Property_Area â€“ Urban/rural,\n",
    "- Loan_Status â€“ ğŸ”‘ Target variable indicating whether the loan was approved or not (used for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e74a7-0e3d-4170-8371-708f11946b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import *\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('datasets/loan_approval_dataset.csv')\n",
    "\n",
    "# view the dataset structure by preview the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121336e-a353-4b1e-a07d-3db97dc3ef9b",
   "metadata": {},
   "source": [
    "## ğŸ§® Preprocessing data\n",
    "Preprocessing steps to prepare dataset for machine learning :\n",
    "- Encode categorical data into numeric form : Machine learning models require numerical input. Therefore, all categorical features such as 'education' and 'self_employed' or 'loan_status' are converted into numeric values using mapping.\n",
    "- Drop non-relevant columns : Columns like 'Loan_id', which serve only as identifiers and do not carry meaningful information for prediction, are removed from the dataset to avoid unnecessary noise in model training.\n",
    "- Split the data into training and testing sets : The dataset is divided into two parts: a training set (typically 80%) used to train the model, and a testing set (typically 20%) used to evaluate how well the model generalizes to new, unseen data.\n",
    "- Separate features from the target variable : The input variables (features) used for making decisions are separated from the target variable (label), which in this case is 'loan_status',\n",
    "- Normalize features : To ensure that all features contribute equally to the learning process, the numeric data is scaled to a standard range (usually between 0 and 1). This improves model convergence and performance, especially for SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e2234-9bf3-49d0-a1fd-b7fc1e1ac2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all categorical data into numeric using map\n",
    "df['education'] = df['education'].map({' Graduate':1,' Not Graduate':0})\n",
    "df['self_employed'] = df['self_employed'].map({' Yes':1,' No':0})\n",
    "df['loan_status'] = df['loan_status'].map({' Approved':1,' Rejected':0})\n",
    "\n",
    "# drop non-relevant columns like Loan_id witch is juste an incremental number\n",
    "df = df.drop(columns=['loan_id'])\n",
    "\n",
    "# Define train/test ratio\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(df) * train_ratio)\n",
    "\n",
    "# Split the data into train and test\n",
    "train = df.iloc[:train_size]\n",
    "test = df.iloc[train_size:]\n",
    "\n",
    "# separate the decision making variables from the final decision in the last column\n",
    "x_train = train.iloc[:,:11]\n",
    "y_train = train.iloc[:,11]\n",
    "x_test = test.iloc[:,:11]\n",
    "y_test = test.iloc[:,11]\n",
    "\n",
    "# Normalize features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# shapes check\n",
    "print('x_train shape : ', x_train_scaled.shape)\n",
    "print('x_test shape : ', x_test_scaled.shape)\n",
    "print('y_train shape : ', y_train.shape)\n",
    "print('y_test shape : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa3dae-3c5c-482c-9dde-b0e73ee6df35",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 2 â€“ Decision tree data classification\n",
    "A Decision Tree breaks down a dataset into smaller and smaller subsets while developing an associated tree of decisions, ultimately leading to a prediction at the leaves.\n",
    "\n",
    "Example :\n",
    "```\n",
    "Is income_annum > 50K?    \n",
    " â”œâ”€â”€ Yes â†’ Is education = Graduate?      \n",
    " â”‚     â”œâ”€â”€ Yes â†’ Approve Loan     \n",
    " â”‚     â””â”€â”€ No â†’ Reject Loan     \n",
    " â””â”€â”€ No â†’ Reject Loan      \n",
    "```\n",
    "Each node in the tree represents a feature (attribute), each branch represents a decision rule, and each leaf represents an outcome or label.\n",
    "\n",
    "The model is trained by feeding the preprocessed data and their corresponding labels. It learns rules that map input features to target classes (e.g., \"Approved\" or \"Rejected\" loans).\n",
    "\n",
    "Once trained, the model is tested on unseen data. Its accuracy is measured by comparing its predictions to the actual labels. A confusion matrix is used to visualize how well the model distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd1001-e1da-4391-9176-4caa0a978e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "tree_model.fit(x_train_scaled, y_train)\n",
    "y_pred_tree = tree_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ab82b-2aa8-41a0-bb3d-61cd382f76a1",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Decision Tree Classification Model\n",
    "\n",
    "Accuracy is the proportion of correctly predicted labels out of all predictions.\n",
    "$$\\text{Accuracy} = \\frac{\\sum \\text{Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by showing the counts of actual vs. predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf048ffc-251d-4a7f-84db-1e52167c1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "correct = np.sum(y_test == y_pred_tree)\n",
    "total = len(y_test)\n",
    "accuracy = correct / total\n",
    "print('Tree Accuracy : ', round(accuracy,2))\n",
    "\n",
    "# compute and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_tree)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree structure\n",
    "tree.plot_tree(tree_model, feature_names=x_train.columns, class_names=['Rejected', 'Approved'], filled=True)\n",
    "plt.title('Decision Tree Structure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d218f7-a02c-47a7-b087-049d7d55e244",
   "metadata": {},
   "source": [
    "### Make decision on new applicant for loan\n",
    "- Create a new applicant (with the same features used during training)\n",
    "- Preprocess this data (scaling)\n",
    "- Use the trained model to predict whether the loan will be approved or rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b56e8-0162-4315-a2af-9d5a89ca27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define applicant data as a row array (1 row, 10 columns)\n",
    "# Example applicant: [no_of_dependents,education,self_employed,income_annum,loan_amount,loan_term,cibil_score,residential_assets_value,commercial_assets_value,luxury_assets_value,bank_asset_value]\n",
    "new_applicant = pd.DataFrame([[2,1,1,9100000,31500000,14,679,10800000,16600000,20900000,5000000]], columns=x_train.columns)\n",
    "\n",
    "# Step 2: Scale the input using the same scaler as during training\n",
    "new_applicant = scaler.transform(new_applicant)\n",
    "\n",
    "# Step 3: Predict using the trained model\n",
    "status = tree_model.predict(new_applicant)\n",
    "\n",
    "# Step 4: Interpret the result\n",
    "print('Loan Status for new applicant : ', status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5a515-af8c-4e91-9cb4-d0f47ee90f3c",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 3 â€“ Support Vector Machine (SVM) data classification\n",
    "A Support Vector Machine (SVM) finds the best boundary (called a hyperplane) that separates different classes in the feature space with the maximum margin â€” the largest possible distance between the hyperplane and the nearest data points from each class (called support vectors).\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine a 2D plot where:\n",
    "- X-axis = applicant income\n",
    "- Y-axis = loan amount\n",
    "\n",
    "SVM will try to draw a straight line that separates Approved from Rejected applicants with the widest possible gap between the two groups.\n",
    "```\n",
    "                     â— (Approved)        \n",
    "                    /         \n",
    "                   /        \n",
    "------------------/ â† Maximum-margin hyperplane        \n",
    "                 /     \n",
    "                /     \n",
    "               â— (Rejected)       \n",
    "```\n",
    "SVM can also handle non-linear relationships using kernel tricks, allowing it to find complex boundaries in higher dimensions.\n",
    "\n",
    "The model is trained to find the optimal hyperplane that separates classes with maximum margin using labeled data. then the model is evaluated on unseen data, and accuracy is measured by comparing predictions to true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699f1ae-ac8b-4536-8182-0e2a4cfab9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVM classifier\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(x_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d0772-c7a7-425c-84cc-ff28c8b63dc0",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Support Vector Machine (SVM) Classification Model\n",
    "As for the decision tree, accuracy for SVM is the proportion of correctly predicted labels out of all predictions.\n",
    "$$\\text{Accuracy} = \\frac{\\sum \\text{Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by showing the counts of actual vs. predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e95c0-58d8-4311-98f9-5ab3d067657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "correct = np.sum(y_test == y_pred_svm)\n",
    "total = len(y_test)\n",
    "accuracy = correct / total\n",
    "print('SVM Accuracy : ', round(accuracy,2))\n",
    "\n",
    "# compute and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd748a8-389a-47d0-8b31-58d6987fabee",
   "metadata": {},
   "source": [
    "### Make decision on new applicant for loan\n",
    "- Create a new applicant (with the same features used during training)\n",
    "- Preprocess this data (scaling)\n",
    "- Use the trained model to predict whether the loan will be approved or rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78ac45-ff28-4548-be39-17c480f5a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define applicant data as a row array (1 row, 10 columns)\n",
    "# example applicant: [no_of_dependents,education,self_employed,income_annum,loan_amount,loan_term,cibil_score,residential_assets_value,commercial_assets_value,luxury_assets_value,bank_asset_value]\n",
    "new_applicant = pd.DataFrame([[2,1,1,9100000,31500000,14,679,10800000,16600000,20900000,5000000]], columns=x_train.columns)\n",
    "\n",
    "# scale the input using the same scaler as during training\n",
    "new_applicant = scaler.transform(new_applicant)\n",
    "\n",
    "# predict using the trained model\n",
    "status = svm_model.predict(new_applicant)\n",
    "\n",
    "# interpret the result\n",
    "print('Loan Status for new applicant : ', status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a64bf6-f319-4869-b9ea-9d663079495d",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Time Series Financial Data for Regression\n",
    "\n",
    "In the context of the time series dataset like stock_details_3_years.csv, regression refers to the task of predicting the future stock closing prices based on various historical financial indicators such as open price.\n",
    "\n",
    "The objective is to build models that learns temporal patterns from past stock performance to generate accurate predictions of future prices.\n",
    "\n",
    "The model serves as a decision support tool to guide trading strategies, risk assessment, and portfolio management by forecasting short-term price trends.\n",
    "\n",
    "ğŸ§  Used Models\n",
    "- Decision Tree Regressor: A rule-based model that segments the input space to minimize variance in predicted stock prices,\n",
    "- Support Vector Regressor (SVR): A powerful kernel-based algorithm that finds a function within a certain margin of tolerance to predict continuous outcomes.\n",
    "\n",
    "ğŸ” Objective: Train and evaluate these models on historical stock data to forecast future price values and enhance data-driven financial decision-making.\n",
    "\n",
    "ğŸ§  Objective:\n",
    "- Step 1 â€“ Load and Preprocess the Time Series Financial Dataset,\n",
    "- Step 2 â€“ Stock Price Prediction using Decision Tree Regression,\n",
    "- Step 3 â€“ Stock Price Prediction using Support Vector Regression (SVR),\n",
    "- Step 4 â€“ (Optional) Forecast with 1D CNN for capturing temporal structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17a5fc-8b4e-48c1-b524-86c645a5c657",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 1 â€“ Load and Preprocess the Time Series Financial Dataset\n",
    "### ğŸ“ˆ About Dataset\n",
    "\n",
    "The stock_details_3_years.csv dataset contains historical stock market data recorded over a three-year period. It includes daily financial indicators that reflect the market behavior of a specific stock or financial asset. The primary objective is to use these historical indicators to predict future closing prices, making it a regression problem.\n",
    "\n",
    "The dataset consists entirely of numerical variables and represents sequential observations ordered by time â€” essential for time series forecasting. The target variable is typically the closing price on each day, and the input is open price from past market activity.\n",
    "\n",
    "Key features in the dataset include:\n",
    "- Open â€“ Opening price of the stock on a given trading day,\n",
    "- Closing Price â€“ ğŸ”‘ Target variable for regression, indicating the final price at market close.\n",
    "\n",
    "This dataset is suitable for training machine learning models that aim to forecast stock prices, aiding in investment decisions and risk management strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f00d2-ea31-40e7-a7e6-322a319f0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import *\n",
    "\n",
    "# load dataset\n",
    "df_ts = pd.read_csv('datasets/stock_details_3_years.csv')\n",
    "\n",
    "# view the dataset structure by preview the first 5 rows\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794c19a-e34d-4926-b673-c125502b91b6",
   "metadata": {},
   "source": [
    "## ğŸ§® Preprocessing Data\n",
    "Preprocessing steps to prepare the time series dataset for machine learning regression:\n",
    "- Drop non-relevant columns: Any columns that do not contribute meaningfully to stock price prediction, such as identifiers or unrelated metadata, are removed to avoid noise during model training.\n",
    "- Sort and structure data chronologically: Since this is time series data, it is essential to ensure that the dataset is sorted by date to preserve the temporal order, which is crucial for accurate forecasting.\n",
    "- Create lag features (optional): To incorporate temporal dependencies, new features like previous closing prices or moving averages can be created to give the model access to historical context.\n",
    "- Split the data into training and testing sets: The dataset is divided into a training set (typically 80%) used to train the model, and a testing set (20%) to evaluate how well it predicts unseen future values.\n",
    "- Separate features from the target variable: Input variables (such as Open, High, Low, Volume) are separated from the target variable, which in this case is the Closing Price to be predicted.\n",
    "- Normalize features: All numerical features are scaled to a consistent range (e.g., between 0 and 1) to ensure that no single feature dominates the learning process. This is particularly important for models like SVR and CNNs, which are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e938d8-68d1-4503-aa1f-6c5f7cb5490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data by column name\n",
    "df_ts_google = df_ts[df_ts['Company'] == 'GOOGL'].copy()\n",
    "\n",
    "# Define train/test ratio\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(df_ts_google) * train_ratio)\n",
    "\n",
    "# Split the data into train and test\n",
    "train = df_ts_google.iloc[:train_size]\n",
    "test = df_ts_google.iloc[train_size:]\n",
    "\n",
    "# separate the decision making variable from the final decision\n",
    "x_train = train.iloc[:,1:2]\n",
    "y_train = train.iloc[:,4]\n",
    "x_test = test.iloc[:,1:2]\n",
    "y_test = test.iloc[:,4]\n",
    "\n",
    "# shapes check\n",
    "print('x_train shape : ', x_train.shape)\n",
    "print('x_test shape : ', x_test.shape)\n",
    "print('y_train shape : ', y_train.shape)\n",
    "print('y_test shape : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32dab2-525e-47c6-a294-1db4e1121e0a",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 2 â€“ Decision Tree Stock Price Regression\n",
    "A Decision Tree Regressor models continuous outcomes by learning decision rules based on the input features. It splits the data into smaller subsets by asking threshold-based questions on features like Open, High, Low, or Volume, and predicts numerical values (e.g., stock closing price) at the leaves.\n",
    "\n",
    "Example :\n",
    "```\n",
    "Is Open > 200?   \n",
    "â”œâ”€â”€ Yes â†’ Is Volume > 1M?   \n",
    "â”‚ â”œâ”€â”€ Yes â†’ Predict Close = 215.3   \n",
    "â”‚ â””â”€â”€ No â†’ Predict Close = 205.7   \n",
    "â””â”€â”€ No â†’ Predict Close = 178.4   \n",
    "```\n",
    "Each node in the tree represents a feature-based split, each branch is a condition or decision rule, and each leaf node holds a numerical prediction (e.g., expected closing stock price).\n",
    "\n",
    "This structure allows the model to learn interpretable and segmented patterns in the data, making it easy to visualize and explain why certain price predictions are made based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3870f-e331-441c-8f1f-73620ce57ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree regression\n",
    "tree_model = tree.DecisionTreeRegressor()\n",
    "tree_model.fit(x_train, y_train)\n",
    "y_pred_tree = tree_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ec62a-c1a9-4b15-971f-cc45519addba",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Decision Tree Regression Model\n",
    "\n",
    "In regression problems, model evaluation is based on how closely the predicted values align with the actual outcomes. Unlike classification tasks, regression does not use accuracy or a confusion matrix, as the outputs are continuous rather than categorical.\n",
    "\n",
    "One widely used metric for evaluating regression models is Mean Absolute Error (MAE). MAE represents the average absolute difference between predicted values and actual values. It gives an intuitive sense of prediction accuracy â€” how far off the model typically is, regardless of direction.\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value,\n",
    "- $n$ is the total number of predictions.\n",
    "\n",
    "This metric is particularly useful in financial forecasting tasks like predicting stock prices or returns, where small errors in predictions can lead to large implications in decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8342b3-1d65-4ba1-97a1-2fc774bcee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MAE\n",
    "absolute_errors = np.abs(y_test - y_pred_tree)\n",
    "mae = np.mean(absolute_errors)\n",
    "print('Tree MAE : ', round(mae,2))\n",
    "\n",
    "# plot the train data\n",
    "plt.plot(train.index, train['Close'], label='Training Data', color='blue')\n",
    "# plot the actual test data\n",
    "plt.plot(test.index, test['Close'], label='Test Data', color='orange')\n",
    "# plot predicted values for the test data\n",
    "plt.plot(test.index, y_pred_tree, label='Predicted Close', color='green', linestyle='--')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Close Price')\n",
    "plt.title('Stock Price Prediction - Google (Train/Test/Prediction)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d466a9b-c655-45b0-a652-d19829fad4b3",
   "metadata": {},
   "source": [
    "### predict close price for a new day using decision tree regression\n",
    "- Create a new data point with Open price,\n",
    "- Use the trained model to predict the Close price for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660aef7-1c6f-4ab7-808f-1c060bd1afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the new input as a DataFrame Open price = 125.75\n",
    "new_day = pd.DataFrame([125.75], columns=x_train.columns)\n",
    "\n",
    "# Step 2: Predict using the trained regression model (e.g., Decision Tree or SVM)\n",
    "predicted_close = tree_model.predict(new_day)\n",
    "\n",
    "# Step 3: Output the predicted Close price\n",
    "print('Predicted Close Price for new day : ', round(predicted_close[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fac5a-1d1b-40bc-b44c-31f621366da4",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 3 â€“ Support Vector Machine (SVM) for Time Series Regression\n",
    "\n",
    "A Support Vector Regression (SVR) is the regression counterpart of SVM. Instead of finding a boundary that separates classes, SVR finds a function that approximates the true relationship between input features and a continuous target value, such that most data points fall within a margin of tolerance (Îµ) from the predicted function.\n",
    "Concept:\n",
    "\n",
    "SVR tries to fit the best flat line or curve (in higher dimensions) through the data while ignoring errors within a certain margin (epsilon-tube). Only data points outside this margin influence the model â€” these are the support vectors.\n",
    "Example:\n",
    "\n",
    "Imagine a plot where:\n",
    "- X-axis = Date or time index\n",
    "- Y-axis = Stock Close price\n",
    "\n",
    "SVR draws a regression line that balances simplicity with accuracy, and only penalizes predictions that fall outside the epsilon margin.\n",
    "```\n",
    "  Close Price\n",
    "      â–²    \n",
    "      â”‚        â—‹    \n",
    "      â”‚      â—‹â—‹â—‹â—‹â—‹    \n",
    "      â”‚     â—‹     â—‹     \n",
    "      â”‚----â—‹-------â—‹----  â† Îµ margin    \n",
    "      â”‚   â—‹         â—‹     \n",
    "      â”‚  â—‹           â—‹     \n",
    "      â”‚      \n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time     \n",
    "\n",
    "        Support Vector Regression Fit\n",
    "```\n",
    "SVR is powerful for forecasting continuous values (like stock prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21aa391-b7fe-4e44-86e0-7b78f89db7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM regression\n",
    "svm_model = svm.SVR()\n",
    "svm_model.fit(x_train, y_train)\n",
    "y_pred_svm = svm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df431a-5f31-415c-b746-c8f577b77af2",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Support Vector Machine (SVM) Regression Model\n",
    "\n",
    "In regression problems, model evaluation is based on how closely the predicted values align with the actual outcomes. Unlike classification tasks, regression does not use accuracy or a confusion matrix, as the outputs are continuous rather than categorical.\n",
    "\n",
    "One widely used metric for evaluating regression models is Mean Absolute Error (MAE). MAE represents the average absolute difference between predicted values and actual values. It gives an intuitive sense of prediction accuracy â€” how far off the model typically is, regardless of direction.\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value,\n",
    "- $n$ is the total number of predictions.\n",
    "\n",
    "This metric is particularly useful in financial forecasting tasks like predicting stock prices or returns, where small errors in predictions can lead to large implications in decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0971340-a7b3-48a7-9656-6bf596947b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MAE\n",
    "absolute_errors = np.abs(y_test - y_pred_svm)\n",
    "mae = np.mean(absolute_errors)\n",
    "print('SVM MAE : ', round(mae,2))\n",
    "\n",
    "# plot the train data\n",
    "plt.plot(train.index, train['Close'], label='Training Data', color='blue')\n",
    "# plot the actual test data\n",
    "plt.plot(test.index, test['Close'], label='Test Data', color='orange')\n",
    "# plot predicted values for the test data\n",
    "plt.plot(test.index, y_pred_svm, label='Predicted Close', color='green', linestyle='--')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Close Price')\n",
    "plt.title('Stock Price Prediction - Google (Train/Test/Prediction)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded538e1-61ad-4ffa-af66-478bd9f6fc14",
   "metadata": {},
   "source": [
    "### predict close price for a new day using decision tree regression\n",
    "- Create a new data point with Open price,\n",
    "- Use the trained model to predict the Close price for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0eb496-dbb0-43c7-ac4a-72b2aca8e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the new input as a DataFrame Open price = 125.75\n",
    "new_day = pd.DataFrame([125.75], columns=x_train.columns)\n",
    "\n",
    "# Step 2: Predict using the trained regression model (e.g., Decision Tree or SVM)\n",
    "predicted_close = svm_model.predict(new_day)\n",
    "\n",
    "# Step 3: Output the predicted Close price\n",
    "print('Predicted Close Price for new day : ', round(predicted_close[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c455a-e581-4a31-bffb-f3939885c484",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Handwritten Digit Classification with MNIST\n",
    "\n",
    "In the context of financial applications, digit recognition plays a critical role in automating tasks like reading handwritten amounts on checks, forms, and financial documents.\n",
    "\n",
    "Financial institutions process millions of handwritten checks and forms. Digitizing and classifying handwritten numeric fields.\n",
    "using Convolutional Neural Network (CNN), which is a deep learning model that automatically extracts spatial hierarchies of features through convolutional layers, ideal for recognizing complex digit patterns.\n",
    "\n",
    "ğŸ” Objective: Train and evaluate the model on the MNIST dataset to accurately classify handwritten digits extracted from scanned financial documents like bank checks, enabling robust digit recognition in automated financial processing systems.\n",
    "\n",
    "ğŸ“Œ Steps Overview:\n",
    "- Step 1 â€“ Load and Preprocess the MNIST Image Dataset,\n",
    "- Step 2 â€“ Digit Classification using Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699b5d8-02ee-4804-bf12-cad712f527f4",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 1 â€“ Load and Preprocess the MNIST Image Dataset\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models designed specifically for image data. They automatically learn spatial hierarchies of features from raw pixels, making them extremely effective for tasks like handwritten digit recognition.\n",
    "\n",
    "In this context, CNNs are applied to the MNIST dataset to classify handwritten digits (0â€“9) with high accuracy. This is especially relevant in financial domains, such as automated check processing or digitized form analysis, where recognizing numeric entries is critical for workflow automation and fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f45746-ccc7-44c9-93f0-9b3f440d5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and dataset\n",
    "from tensorflow import keras\n",
    "from keras import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd85202-5450-42ed-ad7d-9b5610f86cb2",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 2 â€“ Build the CNN and view the structure\n",
    "\n",
    "A CNN is a class of deep learning models specialized for processing data with a grid-like topology, such as images. For digit classification, CNNs excel by automatically learning spatial hierarchies of features (like edges, shapes, and textures) through convolutional layers.\n",
    "\n",
    "The input image passes through a series of layers starting with a convolutional layer to extract features, followed by ReLU activation, max pooling to downsample, global average pooling to reduce dimensions, a fully connected dense layer for higher-level abstraction, and finally a softmax output layer to predict the digit class (0-9).\n",
    "\n",
    "```\n",
    "                         Input Image (28x28 pixels)\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚  Convolution Layer  â”‚ â† Detects edges & curves\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚   MaxPooling Layer  â”‚ â† Reduces spatial dimensions\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚  Convolution Layer  â”‚ â† Learns complex features\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚ GlobalAveragePooling2D     â”‚ â† Flattens feature maps\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚    Dense Layer      â”‚ â† Fully connected, learns digit patterns\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚   Output Layer      â”‚ â† 10 nodes (0â€“9), softmax activation\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                          Predicted Digit Label (e.g. \"5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86023905-c06a-4f32-968d-2aa98837c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple CNN model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    " # vie the structure of the CNN\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bc83c-cc8d-4939-a29d-45aac0c0ddfc",
   "metadata": {},
   "source": [
    "We need to train the model to adjuste the model's learnable parameters, which are initially set randomly, by iteratively minimizing the error between predicted and actual values using optimization algorithms ADAM.\n",
    "While testing involves evaluating the trained model's performance on unseen data to assess its generalization ability by comparing the model's predictions with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cf9cb-fc3e-4978-b6c5-60ba230413ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a741a-9ddc-4da7-b655-6f54595bdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for test set\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# You need to convert the predicted probabilities into class labels before comparing\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "\n",
    "# compute accuracy\n",
    "correct = np.sum(y_test == y_pred_cnn)\n",
    "total = len(y_test)\n",
    "accuracy = correct / total\n",
    "print('CNN Accuracy : ', round(accuracy,2))\n",
    "\n",
    "# compute and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_cnn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('CNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211fcd5-263f-451f-9a49-6a21cd4d5859",
   "metadata": {},
   "source": [
    "## Deeper CNN model\n",
    "\n",
    "By adding more layersâ€”especially convolutional and dense layersâ€”you've increased the model's capacity to learn richer and more complex features from the data, which directly increases the number of trainable parameters:\n",
    "- ğŸ§  More layers â†’ More filters and neurons â†’ More parameters to learn\n",
    "- âœ… This can improve accuracy on complex tasks\n",
    "- âš ï¸ But it also increases risk of overfitting, training time, and computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb28384-237c-4e2c-9da1-213b3da0ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, activation='relu'),  # second conv layer\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d0f5e-87e5-44df-91f0-59b3cb6c2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e99bc-af15-4811-94a7-4759e8400ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for test set\n",
    "y_pred_cnn = model.predict(x_test)\n",
    "\n",
    "# You need to convert the predicted probabilities into class labels before comparing\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "\n",
    "# compute accuracy\n",
    "correct = np.sum(y_test == y_pred_cnn)\n",
    "total = len(y_test)\n",
    "accuracy = correct / total\n",
    "print('CNN Accuracy : ', round(accuracy,2))\n",
    "\n",
    "# compute and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_cnn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('CNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6fc62-4cd1-43e3-8aef-9e982639752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict one image directly from the test set\n",
    "prediction = model.predict(x_test[0:1])  # Predict for the first image, keep it as a batch (shape: (1, 28, 28, 1))\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# display a sample digit image\n",
    "plt.imshow(x_train[0])\n",
    "plt.title(\" True label : \" + str(y_train[0])+ \" Predicted label : \" + str(predicted_label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525bd2c-0946-4791-a7f5-f906045dc96d",
   "metadata": {},
   "source": [
    "## Forged Signature detection\n",
    "\n",
    "In the code above, we compare two signature imagesâ€”one genuine and one forgedâ€”by passing them individually through a trained Convolutional Neural Network (CNN). The model predicts the identity (class) of the person associated with each image. \n",
    "\n",
    "If both images are from the same person, the model should ideally predict the same class label. If the images are from different persons (as in this case), the model should assign different labels. This simple comparison allows us to assess the modelâ€™s ability to differentiate between genuine and forged signatures based on learned visual patterns.\n",
    "\n",
    "```\n",
    "               Signature Image A                     Signature Image B\n",
    "           (e.g., Genuine Signature)             (e.g., Forged Signature)\n",
    "                    â”‚                                     â”‚\n",
    "                    â–¼                                     â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚   Convolutional Neural     â”‚         â”‚   Convolutional Neural     â”‚\n",
    "      â”‚      Network (CNN)         â”‚         â”‚      Network (CNN)         â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚                                     â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                       â–¼\n",
    "                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                            â”‚       Persone1      â”‚ â† Same Label/Different Label\n",
    "                            â”‚                     â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d845d-a2f2-4e1c-9705-ca9a5b051c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "\n",
    "# Define path\n",
    "data_dir = 'datasets/signature'  # Directory with subfolders for each person\n",
    "\n",
    "# Load dataset from directories\n",
    "image_size = (128, 128)  # Resize as needed\n",
    "batch_size = 16\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale'  # Assuming signatures are in grayscale\n",
    ")\n",
    "\n",
    "# Split into train and validation\n",
    "train_ds = dataset.take(24)  # 24 persons for training\n",
    "val_ds = dataset.skip(24)    # 6 persons for validation\n",
    "\n",
    "# Normalize\n",
    "#normalization_layer = keras.layers.Rescaling(1./255)\n",
    "#train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "#val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Build CNN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(128, 128, 1)),\n",
    "\n",
    "    # Block 1\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    # Block 2\n",
    "    keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    # Block 3\n",
    "    keras.layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Dropout(0.4),\n",
    "\n",
    "    # Classification Head\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(30, activation='softmax')  # 30 classes (persons)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "#model.fit(train_ds)\n",
    "model.fit(train_ds,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea834cc-ee57-45f0-a4a9-bed8577acb4d",
   "metadata": {},
   "source": [
    "## Compare Two Signature Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ef1a2-dc72-4ede-ab49-758ff967d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to one images (choose any from dataset)\n",
    "genuine_image_path = 'datasets/signature/person1/NFI-00101001.png'\n",
    "forged_image_path = 'datasets/signature/person2/NFI-00201002.png'\n",
    "\n",
    "# Load image as grayscale\n",
    "genuine_image = keras.utils.load_img(genuine_image_path, color_mode='grayscale', target_size=(128, 128))\n",
    "forged_image = keras.utils.load_img(forged_image_path, color_mode='grayscale', target_size=(128, 128))\n",
    "\n",
    "# Predict one image directly from the test set\n",
    "prediction = model.predict(genuine_image)  # Predict for the first image\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# display a sample digit image\n",
    "plt.imshow(genuine_image)\n",
    "plt.title(\" True label : Persone 1 \" + str(predicted_label))\n",
    "plt.show()\n",
    "\n",
    "prediction = model.predict(forged_image)  # Predict for the seconde image\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "plt.imshow(forged_image)\n",
    "plt.title(\" True label : Persone 2 \" + str(predicted_label))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
